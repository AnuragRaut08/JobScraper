# JobScraper


> An automated, endâ€‘toâ€‘end pipeline that scrapes fresh *Dataâ€‘Engineerâ€‘centric* job postings every hour, filters them for **Hâ€‘1Bâ€‘friendly U.S. companies**, deduplicates & loads them into SQLite, and exports handy CSV/JSON/HTML reports â€“ all with a single command.

---

## âœ¨ Key Features

| Step                 | What it Does                                                                                                                        | Tech                                            |
| -------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- |
| **1. Web Scraping**  | Pulls jobs posted *within the last hour* from **LinkedIn** (Selenium) and optional **GoogleÂ Jobs** (SerpAPI) & *fallback* RemoteOK. | `selenium`, `google-search-results`             |
| **2. Visa Filter**   | Crossâ€‘references jobs with **21,114** companies that filed Hâ€‘1B petitions in the last 3Â years.                                      | `fuzzywuzzy`Â (+Â `pythonâ€‘Levenshtein` for speed) |
| **3. Database Load** | Stores cleaned data in **SQLite** with a strong `UNIQUE` constraint to prevent duplicates.                                          | `sqlite3`, `pandas`                             |
| **4. Automation**    | `python main.py schedule` runs the whole flow hourly. Drop one cron/Taskâ€‘Scheduler line â€“ done.                                     | plainÂ Cron / TaskÂ Scheduler                     |
| **5. Reporting**     | Autoâ€‘exports **CSV, JSON, HTML**, plus a mini **market analysis** (top companies, titles, locations).                               | `pandas`, `collections.Counter`                 |

---

## ğŸ“‚ Project Structure

```
jobscraper/
â”œâ”€ main.py                  # CLI entryâ€‘point (run / test / interactive / schedule)
â”œâ”€ pipeline.py              # Orchestrates scraping â†’ filter â†’ DB â†’ export
â”œâ”€ db/
â”‚   â””â”€ job_database.py      # SQLite schema, insert, stats, cleanup
â”œâ”€ scrapers/
â”‚   â”œâ”€ linkedin.py          # Seleniumâ€‘based LinkedIn scraper
â”‚   â”œâ”€ google_jobs.py       # SerpAPI Google Jobs scraper (optional)
â”‚   â””â”€ remoteok.py          # Lightweight JSON API fallback
â”œâ”€ data/
â”‚   â””â”€ h1b_sponsors.txt     # 21k+ visaâ€‘friendly companies
â””â”€ README.md
```

---

## âš¡ QuickÂ Start

```bash
# 1âƒ£  Clone repo & enter env
$ git clone https://github.com/<yourâ€‘user>/jobscraper.git
$ cd jobscraper

# 2âƒ£  Install deps
$ pip install -r requirements.txt  # PythonÂ 3.10+

# 3âƒ£  (Optional) add Google Jobs key
$ export SERPAPI_API_KEY="<yourâ€‘serpapiâ€‘key>"

# 4âƒ£  Oneâ€‘shot scrape + load
$ python main.py run

# 5âƒ£  Interactive search (any keywords / location)
$ python main.py interactive

# 6âƒ£  Hourly automation (runs forever)
$ python main.py schedule
```

> **Windows users**: replace `export` with `setx` or use TaskÂ Scheduler.

---

## âš™ï¸ CLIÂ Modes

| Command                      | Purpose                                                  |
| ---------------------------- | -------------------------------------------------------- |
| `python main.py test`        | Smokeâ€‘tests scrapers and DB init without inserting data. |
| `python main.py run`         | Full scrape âœ visa filter âœ DB load âœ export once.       |
| `python main.py interactive` | Promptâ€‘driven search (keywords, location, source).       |
| `python main.py schedule`    | Infinite hourly loop; logs each run and deduplicates.    |

### Scheduling with Cron (Unix/macOS)

```cron
0 * * * * /usr/bin/python3 /path/to/jobscraper/main.py schedule >> /var/log/jobscraper.log 2>&1
```

### Scheduling with WindowsÂ TaskÂ Scheduler

1. *Create Basic Task* â†’ **Trigger**: Daily, Recur every 1 day, Repeat task every 1Â hour.
2. **Action**: *Start a Program* â†’ `python`
   **AddÂ args**: `main.py schedule`
   **StartÂ in**: `C:\Users\<you>\visa_scraper`

---

## ğŸ—„ï¸ SQLite Schema

```sql
CREATE TABLE jobs (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  company_name   TEXT NOT NULL,
  job_title      TEXT NOT NULL,
  posting_time   TEXT,
  job_location   TEXT,
  job_type       TEXT,
  job_description TEXT,
  work_setting   TEXT,
  ats_apply_link TEXT,
  scraped_at     TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  UNIQUE(company_name, job_title, job_location, posting_time)
);
```

* **Indexes** on `(company_name, job_title)` and `scraped_at` keep queries snappy.

---

## ğŸ”‘ EnvironmentÂ Variables

| Variable          | Purpose                                                               |
| ----------------- | --------------------------------------------------------------------- |
| `SERPAPI_API_KEY` | (Optional) Enables GoogleÂ Jobs scraping. Free tier â†’ 100 searches/mo. |

---

## ğŸ“Š Sample Marketâ€‘Analysis Output

```
ğŸ† Most Common Job Titles:
   data (8) â€¢ engineer (6) â€¢ analyst (4) ...
ğŸ¢ Top Companies:
   Stripe (1) â€¢ Postman (1) â€¢ PepsiCo (1) ...
ğŸ“ Locations:
   NewÂ YorkÂ NY (3) â€¢ SanÂ FranciscoÂ CA (2) ...
```

---

## ğŸ¥ Demo Video

[â–¶ï¸ **Watch the 5â€‘minute walkthrough**](https://youtu.be/your_demo_link)

---

## ğŸš§ Known Limitations

* Indeed blocks with **HTTPÂ 403**; handled gracefully but yields 0 jobs without a proxy.
* Google Jobs needs a SerpAPI key; otherwise falls back to direct scraping which may timeâ€‘out.
* Selenium scrapes \~20 jobs/min; heavy use may hit LinkedIn rate limits.

---

## ğŸ› ï¸ Future Improvements

* Swap SQLite â†’ PostgreSQL for multiâ€‘user deployments.
* Prefect or Airflow DAG for cloudâ€‘native scheduling.
* Dockerfile & CI pipeline for oneâ€‘command provisioning.
* Telegram/Slack alerts for fresh matched jobs.

---

## ğŸ“„ License

MITÂ Â©Â 2025Â AnuragÂ [anuragtraut2003@gmail.com](mailto:anuragtraut2003@gmail.com)
